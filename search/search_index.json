{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OpenStreetMap Imports preparations \u00b6 This is a proof of concept and a pet project for learning python while preparing data for importing it to OpenStreetMap. Although I will start small (importing trees from Barcelona city council), I aim to set the foundations for adding other types of imports that may (or may not) be added in the future. Documentation \u00b6 Project's documentation can be found in docs folder and a live version, generated using mkdocs can be found here Contributors \u2728 \u00b6 Thanks goes to these wonderful people ( emoji key ): Carlos C\u00e1mara \ud83d\udcbb Miguel Sevilla-Callejo \ud83d\udcd6 This project follows the all-contributors specification. Contributions of any kind welcome! Project Organization \u00b6 \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile <- Makefile with commands like `make data` or `make train` \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third party sources. \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling. \u2502 \u2514\u2500\u2500 raw <- The original, immutable data dump. \u2502 \u251c\u2500\u2500 docs <- A default Sphinx project; see sphinx-doc.org for details \u2502 \u251c\u2500\u2500 models <- Trained and serialized models, model predictions, or model summaries \u2502 \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a number (for ordering), \u2502 the creator's initials, and a short `-` delimited description, e.g. \u2502 `1.0-jqp-initial-data-exploration`. \u2502 \u251c\u2500\u2500 references <- Data dictionaries, manuals, and all other explanatory materials. \u2502 \u251c\u2500\u2500 reports <- Generated analysis as HTML, PDF, LaTeX, etc. \u2502 \u2514\u2500\u2500 figures <- Generated graphics and figures to be used in reporting \u2502 \u251c\u2500\u2500 environment.yml <- The environment file for reproducing the analysis environment, e.g. \u2502 `conda activate osm_imports_preparations` \u2502 \u251c\u2500\u2500 setup.py <- makes project pip installable (pip install -e .) so src can be imported \u2514\u2500\u2500 src <- Source code for use in this project. \u251c\u2500\u2500 __init__.py <- Makes src a Python module \u2502 \u251c\u2500\u2500 data <- Scripts to download or generate data \u2502 \u2514\u2500\u2500 make_dataset.py \u2502 \u251c\u2500\u2500 features <- Scripts to turn raw data into features for modeling \u2502 \u2514\u2500\u2500 build_features.py \u2502 \u251c\u2500\u2500 models <- Scripts to train models and then use trained models to make \u2502 \u2502 predictions \u2502 \u251c\u2500\u2500 predict_model.py \u2502 \u2514\u2500\u2500 train_model.py \u2502 \u2514\u2500\u2500 visualization <- Scripts to create exploratory and results oriented visualizations \u2514\u2500\u2500 visualize.py Project based on the cookiecutter data science project template . #cookiecutterdatascience","title":"Home"},{"location":"#openstreetmap-imports-preparations","text":"This is a proof of concept and a pet project for learning python while preparing data for importing it to OpenStreetMap. Although I will start small (importing trees from Barcelona city council), I aim to set the foundations for adding other types of imports that may (or may not) be added in the future.","title":"OpenStreetMap Imports preparations"},{"location":"#documentation","text":"Project's documentation can be found in docs folder and a live version, generated using mkdocs can be found here","title":"Documentation"},{"location":"#contributors","text":"Thanks goes to these wonderful people ( emoji key ): Carlos C\u00e1mara \ud83d\udcbb Miguel Sevilla-Callejo \ud83d\udcd6 This project follows the all-contributors specification. Contributions of any kind welcome!","title":"Contributors \u2728"},{"location":"#project-organization","text":"\u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile <- Makefile with commands like `make data` or `make train` \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third party sources. \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling. \u2502 \u2514\u2500\u2500 raw <- The original, immutable data dump. \u2502 \u251c\u2500\u2500 docs <- A default Sphinx project; see sphinx-doc.org for details \u2502 \u251c\u2500\u2500 models <- Trained and serialized models, model predictions, or model summaries \u2502 \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a number (for ordering), \u2502 the creator's initials, and a short `-` delimited description, e.g. \u2502 `1.0-jqp-initial-data-exploration`. \u2502 \u251c\u2500\u2500 references <- Data dictionaries, manuals, and all other explanatory materials. \u2502 \u251c\u2500\u2500 reports <- Generated analysis as HTML, PDF, LaTeX, etc. \u2502 \u2514\u2500\u2500 figures <- Generated graphics and figures to be used in reporting \u2502 \u251c\u2500\u2500 environment.yml <- The environment file for reproducing the analysis environment, e.g. \u2502 `conda activate osm_imports_preparations` \u2502 \u251c\u2500\u2500 setup.py <- makes project pip installable (pip install -e .) so src can be imported \u2514\u2500\u2500 src <- Source code for use in this project. \u251c\u2500\u2500 __init__.py <- Makes src a Python module \u2502 \u251c\u2500\u2500 data <- Scripts to download or generate data \u2502 \u2514\u2500\u2500 make_dataset.py \u2502 \u251c\u2500\u2500 features <- Scripts to turn raw data into features for modeling \u2502 \u2514\u2500\u2500 build_features.py \u2502 \u251c\u2500\u2500 models <- Scripts to train models and then use trained models to make \u2502 \u2502 predictions \u2502 \u251c\u2500\u2500 predict_model.py \u2502 \u2514\u2500\u2500 train_model.py \u2502 \u2514\u2500\u2500 visualization <- Scripts to create exploratory and results oriented visualizations \u2514\u2500\u2500 visualize.py Project based on the cookiecutter data science project template . #cookiecutterdatascience","title":"Project Organization"},{"location":"bcn_trees/","text":"Barcelona's Trees Import documentation \u00b6 Goals \u00b6 The goal is to manually merge and import all the trees' information provided by Barcelona City Council, while testing the scripts for data preparation. Data Sources \u00b6 Two datasets provided by Barcelona City Council will be used: Arbrat viari : Name of the species and geolocation of the trees of the city of Barcelona located on public roads. The information contains, among other data, the scientific name, the common name, the height, the direction and the width of the sidewalk... The trees of the parks are not included. The coordinates are expressed in the ETRS89 reference system. This dataset complemens of Zone trees of the city of Barcelona. Historical resources which contain data available until the last week of the term are published. The resources are ordered by year and term, information that can be found in the name of the resource. Arbrat zona : Name of the species and geolocation of the trees of the city of Barcelona located on public roads. The information contains, among other data, the scientific name, the common name, the height, the direction and the width of the sidewalk... The trees of the parks are not included. The coordinates are expressed in the ETRS89 reference system. This dataset complemens of Street trees of the city of Barcelona. Historical resources which contain data available until the last week of the term are published. The resources are ordered by year and term, information that can be found in the name of the resource. Licence \u00b6 We have an express authorization from the Barcelona city council for the reuse of open data published on theirs open Government website Import type \u00b6 This import will be done manually, using JOSM to edit the data. Consider using Task Manager. Data preparations \u00b6 All data preparations will be made automatically by the scripts within this repository. More specifically from import_bcn_trees.py , which in turn imports the functions defined in src/features/bcn_trees.py aimed to convert original tagging into OSM tagging (for a complete documentation of those functions, please refer to the API Documentation ). Tagging Conversion table: Original field Description OSM tagging Comments CODI Internal ID Not currently imported Consider importing it, as it may be useful for future data updates. X_ETRS89 X coordinates, ETRS89 format Not imported Y_ETRS89 Y coordinates, ETRS89 format Not imported LATITUD_WGS84 Latitude coordinates, WGS84 format Geometry information. No tagging used. LONGITUD_WGS84 Longitude coordinates, WGS84 format Geometry information. No tagging used. TIPUS_ELEMENT Object's type (viari/zona) Not imported ESPAI VERD Name of Green space where the tree is located Not imported ADRECA Address Not imported ALCADA Tree's height. It does not use meters, but categories height height is calculated according to tree's category, following this documentation (p. 22). CAT_ESPECIE_ID Species' ID Not imported NOM_CIENTIFIC scientific name of the species (popularly known as the Latin name) species NOM_CASTELLA Name in Spanish species:es NOM_CATALA Name in Catalan species:ca CATEGORIA_ARBRAT Tree's category. Internal classification according to height and diameter. diameter Not directly imported, but used to calculate height and diameter , following this documentation (p. 19 and 22). AMPLADA_VORERA Sidewalk's width Not imported. DATA_PLANTACIO Date in which the tree was planted planted_date TIPUS_AIGUA Water type Not imported. TIPUS_REG Watering mechanism Not imported. TIPUS_SUPERFICIE Surface type Not imported. TIPUS_SUPORT Support type Not imported. COBERTURA_ESCOCELL Whether the Tree pit is covered or not Not currently imported due to lack of specific tagging, but it would be a nice have to feature. MIDA_ESCOCELL Tree pit Size Not currently imported due to lack of specific tagging, but it would be a nice have to feature. Also, a source=Opendata Ajuntament de Barcelona will be added automatically to all the trees. Import workflow \u00b6 Run import_bcn_trees.py to prepare a single CSV file with all the converted tagging, as specified before. This step only needs to be run once. Use the CSV file generated in the previous step as datasource. Create a project in task manager (TODO) Changesets' tagging \u00b6 We will use the following changeset tags: comment=#Ajuntament de Barcelona Trees' import import=yes source=Ajuntament de Barcelona source:date=* the same that in the dataset url= (this page) References \u00b6 Import Guidelines Procedimiento para preparar importaciones (in Spanish) Sample import documentation project (thanks Lanxana!)","title":"Barcelona's Trees Import"},{"location":"bcn_trees/#barcelonas-trees-import-documentation","text":"","title":"Barcelona's Trees Import documentation"},{"location":"bcn_trees/#goals","text":"The goal is to manually merge and import all the trees' information provided by Barcelona City Council, while testing the scripts for data preparation.","title":"Goals"},{"location":"bcn_trees/#data-sources","text":"Two datasets provided by Barcelona City Council will be used: Arbrat viari : Name of the species and geolocation of the trees of the city of Barcelona located on public roads. The information contains, among other data, the scientific name, the common name, the height, the direction and the width of the sidewalk... The trees of the parks are not included. The coordinates are expressed in the ETRS89 reference system. This dataset complemens of Zone trees of the city of Barcelona. Historical resources which contain data available until the last week of the term are published. The resources are ordered by year and term, information that can be found in the name of the resource. Arbrat zona : Name of the species and geolocation of the trees of the city of Barcelona located on public roads. The information contains, among other data, the scientific name, the common name, the height, the direction and the width of the sidewalk... The trees of the parks are not included. The coordinates are expressed in the ETRS89 reference system. This dataset complemens of Street trees of the city of Barcelona. Historical resources which contain data available until the last week of the term are published. The resources are ordered by year and term, information that can be found in the name of the resource.","title":"Data Sources"},{"location":"bcn_trees/#licence","text":"We have an express authorization from the Barcelona city council for the reuse of open data published on theirs open Government website","title":"Licence"},{"location":"bcn_trees/#import-type","text":"This import will be done manually, using JOSM to edit the data. Consider using Task Manager.","title":"Import type"},{"location":"bcn_trees/#data-preparations","text":"All data preparations will be made automatically by the scripts within this repository. More specifically from import_bcn_trees.py , which in turn imports the functions defined in src/features/bcn_trees.py aimed to convert original tagging into OSM tagging (for a complete documentation of those functions, please refer to the API Documentation ). Tagging Conversion table: Original field Description OSM tagging Comments CODI Internal ID Not currently imported Consider importing it, as it may be useful for future data updates. X_ETRS89 X coordinates, ETRS89 format Not imported Y_ETRS89 Y coordinates, ETRS89 format Not imported LATITUD_WGS84 Latitude coordinates, WGS84 format Geometry information. No tagging used. LONGITUD_WGS84 Longitude coordinates, WGS84 format Geometry information. No tagging used. TIPUS_ELEMENT Object's type (viari/zona) Not imported ESPAI VERD Name of Green space where the tree is located Not imported ADRECA Address Not imported ALCADA Tree's height. It does not use meters, but categories height height is calculated according to tree's category, following this documentation (p. 22). CAT_ESPECIE_ID Species' ID Not imported NOM_CIENTIFIC scientific name of the species (popularly known as the Latin name) species NOM_CASTELLA Name in Spanish species:es NOM_CATALA Name in Catalan species:ca CATEGORIA_ARBRAT Tree's category. Internal classification according to height and diameter. diameter Not directly imported, but used to calculate height and diameter , following this documentation (p. 19 and 22). AMPLADA_VORERA Sidewalk's width Not imported. DATA_PLANTACIO Date in which the tree was planted planted_date TIPUS_AIGUA Water type Not imported. TIPUS_REG Watering mechanism Not imported. TIPUS_SUPERFICIE Surface type Not imported. TIPUS_SUPORT Support type Not imported. COBERTURA_ESCOCELL Whether the Tree pit is covered or not Not currently imported due to lack of specific tagging, but it would be a nice have to feature. MIDA_ESCOCELL Tree pit Size Not currently imported due to lack of specific tagging, but it would be a nice have to feature. Also, a source=Opendata Ajuntament de Barcelona will be added automatically to all the trees.","title":"Data preparations"},{"location":"bcn_trees/#import-workflow","text":"Run import_bcn_trees.py to prepare a single CSV file with all the converted tagging, as specified before. This step only needs to be run once. Use the CSV file generated in the previous step as datasource. Create a project in task manager (TODO)","title":"Import workflow"},{"location":"bcn_trees/#changesets-tagging","text":"We will use the following changeset tags: comment=#Ajuntament de Barcelona Trees' import import=yes source=Ajuntament de Barcelona source:date=* the same that in the dataset url= (this page)","title":"Changesets' tagging"},{"location":"bcn_trees/#references","text":"Import Guidelines Procedimiento para preparar importaciones (in Spanish) Sample import documentation project (thanks Lanxana!)","title":"References"},{"location":"setup/","text":"Project Setup \u00b6 This project uses virtual environments (see below) to handle all dependencies and follows cookiecutter datascience structure to keep the repo clean and tidy. Virtual environments \u00b6 Virtual environments are managed by conda , which means that you should have Anaconda distribution installed (Read installing instructions on their website ) Activate virtual environment conda activate osm_imports_preparations Deactivate virtual environment: conda deactivate Update virtual environment from environment.yml : conda env update -f environment.yml Recreate virtual environment from environment.yml : conda env create -f environment.yml Project Organization \u00b6 \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile <- Makefile with commands like `make data` or `make train` \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third party sources. \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling. \u2502 \u2514\u2500\u2500 raw <- The original, immutable data dump. \u2502 \u251c\u2500\u2500 docs <- A default Sphinx project; see sphinx-doc.org for details \u2502 \u251c\u2500\u2500 models <- Trained and serialized models, model predictions, or model summaries \u2502 \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a number (for ordering), \u2502 the creator's initials, and a short `-` delimited description, e.g. \u2502 `1.0-jqp-initial-data-exploration`. \u2502 \u251c\u2500\u2500 references <- Data dictionaries, manuals, and all other explanatory materials. \u2502 \u251c\u2500\u2500 reports <- Generated analysis as HTML, PDF, LaTeX, etc. \u2502 \u2514\u2500\u2500 figures <- Generated graphics and figures to be used in reporting \u2502 \u251c\u2500\u2500 requirements.txt <- The requirements file for reproducing the analysis environment, e.g. \u2502 generated with `pip freeze > requirements.txt` \u2502 \u251c\u2500\u2500 setup.py <- makes project pip installable (pip install -e .) so src can be imported \u251c\u2500\u2500 src <- Source code for use in this project. \u2502 \u251c\u2500\u2500 __init__.py <- Makes src a Python module \u2502 \u2502 \u2502 \u251c\u2500\u2500 data <- Scripts to download or generate data \u2502 \u2502 \u2514\u2500\u2500 make_dataset.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 features <- Scripts to turn raw data into features for modeling \u2502 \u2502 \u2514\u2500\u2500 build_features.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 models <- Scripts to train models and then use trained models to make \u2502 \u2502 \u2502 predictions \u2502 \u2502 \u251c\u2500\u2500 predict_model.py \u2502 \u2502 \u2514\u2500\u2500 train_model.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 visualization <- Scripts to create exploratory and results oriented visualizations \u2502 \u2514\u2500\u2500 visualize.py \u2502 \u2514\u2500\u2500 tox.ini <- tox file with settings for running tox; see tox.testrun.org Project based on the cookiecutter data science project template . #cookiecutterdatascience Updating Documentation \u00b6 Documentation is generated using MkDocs with some plugins installed (they will all be installed after creating the virtual environment). The live documentation site retrieves two type of documentation: Static, manually created *.md files: these documents are manually created and stored in /docs folder. Automatically generated (API): these documents are generated using keras-audoc , which reads inline functions' comments and generates files according to autogen.py . (currently not working -visit: https://github.com/keras-team/keras-autodoc/issues/68) In turn, menu (amongst other things) is defined in mkdocs.yml file. In order to improve documentation you can do the following: Editing an existing .md file \u00b6 Open the desired .md file within /docs folder and edit it normally. Save your changes Run mkdocs serve and watch your changes in http://127.0.0.1:8000 Once, you're done editing, press CTRL-C to stop the server If you're happy with your changes, run mkdocs gh-deploy : a site will be generated and pushed to gh-pages branch within this repo. Soon after it will be visible in the live site. Creating a new .md file \u00b6 Create a .md file within /docs folder and edit it normally. Save your changes Edit mkdocs.yml and add a menu entry pointing to the newly created file. Run mkdocs serve and watch your changes in http://127.0.0.1:8000 Once, you're done editing, press CTRL-C to stop the server If you're happy with your changes, run mkdocs gh-deploy : a site will be generated and pushed to gh-pages branch within this repo. Soon after it will be visible in the live site.","title":"Project Setup"},{"location":"setup/#project-setup","text":"This project uses virtual environments (see below) to handle all dependencies and follows cookiecutter datascience structure to keep the repo clean and tidy.","title":"Project Setup"},{"location":"setup/#virtual-environments","text":"Virtual environments are managed by conda , which means that you should have Anaconda distribution installed (Read installing instructions on their website ) Activate virtual environment conda activate osm_imports_preparations Deactivate virtual environment: conda deactivate Update virtual environment from environment.yml : conda env update -f environment.yml Recreate virtual environment from environment.yml : conda env create -f environment.yml","title":"Virtual environments"},{"location":"setup/#project-organization","text":"\u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile <- Makefile with commands like `make data` or `make train` \u251c\u2500\u2500 README.md <- The top-level README for developers using this project. \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 external <- Data from third party sources. \u2502 \u251c\u2500\u2500 interim <- Intermediate data that has been transformed. \u2502 \u251c\u2500\u2500 processed <- The final, canonical data sets for modeling. \u2502 \u2514\u2500\u2500 raw <- The original, immutable data dump. \u2502 \u251c\u2500\u2500 docs <- A default Sphinx project; see sphinx-doc.org for details \u2502 \u251c\u2500\u2500 models <- Trained and serialized models, model predictions, or model summaries \u2502 \u251c\u2500\u2500 notebooks <- Jupyter notebooks. Naming convention is a number (for ordering), \u2502 the creator's initials, and a short `-` delimited description, e.g. \u2502 `1.0-jqp-initial-data-exploration`. \u2502 \u251c\u2500\u2500 references <- Data dictionaries, manuals, and all other explanatory materials. \u2502 \u251c\u2500\u2500 reports <- Generated analysis as HTML, PDF, LaTeX, etc. \u2502 \u2514\u2500\u2500 figures <- Generated graphics and figures to be used in reporting \u2502 \u251c\u2500\u2500 requirements.txt <- The requirements file for reproducing the analysis environment, e.g. \u2502 generated with `pip freeze > requirements.txt` \u2502 \u251c\u2500\u2500 setup.py <- makes project pip installable (pip install -e .) so src can be imported \u251c\u2500\u2500 src <- Source code for use in this project. \u2502 \u251c\u2500\u2500 __init__.py <- Makes src a Python module \u2502 \u2502 \u2502 \u251c\u2500\u2500 data <- Scripts to download or generate data \u2502 \u2502 \u2514\u2500\u2500 make_dataset.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 features <- Scripts to turn raw data into features for modeling \u2502 \u2502 \u2514\u2500\u2500 build_features.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 models <- Scripts to train models and then use trained models to make \u2502 \u2502 \u2502 predictions \u2502 \u2502 \u251c\u2500\u2500 predict_model.py \u2502 \u2502 \u2514\u2500\u2500 train_model.py \u2502 \u2502 \u2502 \u2514\u2500\u2500 visualization <- Scripts to create exploratory and results oriented visualizations \u2502 \u2514\u2500\u2500 visualize.py \u2502 \u2514\u2500\u2500 tox.ini <- tox file with settings for running tox; see tox.testrun.org Project based on the cookiecutter data science project template . #cookiecutterdatascience","title":"Project Organization"},{"location":"setup/#updating-documentation","text":"Documentation is generated using MkDocs with some plugins installed (they will all be installed after creating the virtual environment). The live documentation site retrieves two type of documentation: Static, manually created *.md files: these documents are manually created and stored in /docs folder. Automatically generated (API): these documents are generated using keras-audoc , which reads inline functions' comments and generates files according to autogen.py . (currently not working -visit: https://github.com/keras-team/keras-autodoc/issues/68) In turn, menu (amongst other things) is defined in mkdocs.yml file. In order to improve documentation you can do the following:","title":"Updating Documentation"},{"location":"setup/#editing-an-existing-md-file","text":"Open the desired .md file within /docs folder and edit it normally. Save your changes Run mkdocs serve and watch your changes in http://127.0.0.1:8000 Once, you're done editing, press CTRL-C to stop the server If you're happy with your changes, run mkdocs gh-deploy : a site will be generated and pushed to gh-pages branch within this repo. Soon after it will be visible in the live site.","title":"Editing an existing .md file"},{"location":"setup/#creating-a-new-md-file","text":"Create a .md file within /docs folder and edit it normally. Save your changes Edit mkdocs.yml and add a menu entry pointing to the newly created file. Run mkdocs serve and watch your changes in http://127.0.0.1:8000 Once, you're done editing, press CTRL-C to stop the server If you're happy with your changes, run mkdocs gh-deploy : a site will be generated and pushed to gh-pages branch within this repo. Soon after it will be visible in the live site.","title":"Creating a new .md file"}]}